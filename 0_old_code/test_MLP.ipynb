{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from opart_functions import SquaredHingeLoss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from opart_functions import get_acc_rate, get_err_df, gen_data_dict, tune_lldas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "features_fold1_path = 'training_data/genome/seq_features.csv'\n",
    "features_fold2_path = 'training_data/genome/seq_features.csv'  \n",
    "target_fold1_path = 'training_data/genome/target_fold1.csv'\n",
    "target_fold2_path = 'training_data/genome/target_fold2.csv'\n",
    "\n",
    "# sequences and labels\n",
    "seqs_path   = 'raw_data/genome/signals.csv'\n",
    "labels_path = 'raw_data/genome/labels.csv'\n",
    "\n",
    "# err for each log_lambda\n",
    "err_fold1_path = 'training_data/genome/errors_fold1.csv'\n",
    "err_fold2_path = 'training_data/genome/errors_fold2.csv'\n",
    "\n",
    "# writing accuracy rate path\n",
    "acc_rate_path = 'acc_rate/genome.csv'\n",
    "\n",
    "# path to write df to csv\n",
    "output_df_path = 'record_dataframe/genome/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sequence and label dictionary\n",
    "seqs_dict   = gen_data_dict(seqs_path)\n",
    "labels_dict = gen_data_dict(labels_path)\n",
    "\n",
    "# getting dataframe of error count for each log_lambda\n",
    "err_fold1_df = pd.read_csv(err_fold1_path)\n",
    "err_fold2_df = pd.read_csv(err_fold2_path)\n",
    "\n",
    "# features_df\n",
    "features_df_fold1 = pd.read_csv(features_fold1_path)\n",
    "features_df_fold2 = pd.read_csv(features_fold2_path)\n",
    "\n",
    "# targets_df\n",
    "target_df_fold1 = pd.read_csv(target_fold1_path)\n",
    "target_df_fold2 = pd.read_csv(target_fold2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_low_1  = torch.Tensor(target_df_fold1.iloc[:, 1:2].to_numpy())\n",
    "targets_high_1 = torch.Tensor(target_df_fold1.iloc[:, 2:3].to_numpy())\n",
    "targets_low_2  = torch.Tensor(target_df_fold2.iloc[:, 1:2].to_numpy())\n",
    "targets_high_2 = torch.Tensor(target_df_fold2.iloc[:, 2:3].to_numpy())\n",
    "\n",
    "target_fold1 = torch.cat((targets_low_1, targets_high_1), dim=1)\n",
    "target_fold2 = torch.cat((targets_low_2, targets_high_2), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, hidden_size):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.hidden_layers == 0:\n",
    "            self.linear_model = nn.Linear(input_size, 1)\n",
    "        else:\n",
    "            self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "            self.hidden = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(hidden_layers - 1)])\n",
    "            self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            init.constant_(param, 0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.hidden_layers == 0:\n",
    "            return self.linear_model(x)\n",
    "        else:\n",
    "            x = torch.relu(self.input_layer(x))\n",
    "            for layer in self.hidden:\n",
    "                x = torch.relu(layer(x))\n",
    "            x = self.output_layer(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_model(input_size, hidden_layers, hidden_size, batch_size, feature, targets, test_fold, seqs_dict, labels_dict, err_df, n_ites=1):\n",
    "    torch.manual_seed(123)\n",
    "    # prepare training dataset\n",
    "    dataset    = TensorDataset(feature, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size)\n",
    "\n",
    "    # Instantiate model, loss function and opimizer\n",
    "    model = MLPModel(input_size, hidden_layers, hidden_size)\n",
    "    criterion = SquaredHingeLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training loop\n",
    "    rates = []\n",
    "    for i in range(n_ites + 1):\n",
    "        total_loss = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            lldas = model(feature).numpy().reshape(-1)\n",
    "        lldas = tune_lldas(lldas)\n",
    "        \n",
    "        if(i%1 == 0):\n",
    "            df = get_err_df(lldas, test_fold, seqs_dict, labels_dict, err_df)\n",
    "            rate = get_acc_rate(df)\n",
    "            rates.append(rate)\n",
    "            print(i, total_loss/len(dataloader), rate)\n",
    "    \n",
    "    return rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature\n",
    "chosen_feature = ['std_deviation', 'length', 'sum_diff', 'range_value', 'abs_skewness']\n",
    "X = features_df_fold1.iloc[:, 1:][chosen_feature].to_numpy()\n",
    "X0 = X[:, 0]\n",
    "X0 = np.log(X0).reshape(-1, 1)\n",
    "X1 = X[:, 1]\n",
    "X1 = np.log(np.log(X1)).reshape(-1, 1)\n",
    "X2 = X[:, 2]\n",
    "X2 = np.log(np.log(X2)).reshape(-1, 1)\n",
    "X3 = X[:, 3]\n",
    "X3 = np.log(X3).reshape(-1, 1)\n",
    "X4 = X[:, 4]\n",
    "X4 = np.log(X4).reshape(-1, 1)\n",
    "\n",
    "X = np.concatenate([X0, X1, X2, X3, X4], axis=1)\n",
    "mean = np.mean(X, axis=0)\n",
    "std_dev = np.std(X, axis=0)\n",
    "X = (X-mean)/std_dev\n",
    "X = torch.Tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.6106668896338965 70.34574468085107\n",
      "1 1.750301941467994 75.93085106382979\n",
      "2 1.4002043309624648 79.25531914893617\n",
      "3 1.2817521505981664 80.71808510638297\n",
      "4 1.2339965331524902 80.98404255319149\n",
      "5 1.2152120441692391 80.71808510638297\n",
      "6 1.2090856607559497 80.58510638297872\n",
      "7 1.2063345377845227 80.71808510638297\n",
      "8 1.205349632886509 80.58510638297872\n",
      "9 1.2051810882988563 80.58510638297872\n",
      "10 1.2045974557458707 80.85106382978724\n",
      "11 1.2052642586791322 80.58510638297872\n",
      "12 1.2045526792412777 80.71808510638297\n",
      "13 1.2045702969357737 80.85106382978724\n",
      "14 1.2052218727787816 80.71808510638297\n",
      "15 1.2044591433496663 80.85106382978724\n",
      "16 1.204451192008827 80.85106382978724\n",
      "17 1.2050861298110944 80.71808510638297\n",
      "18 1.2043105521130932 80.85106382978724\n",
      "19 1.20429711878817 80.85106382978724\n",
      "20 1.2042668101050515 80.71808510638297\n"
     ]
    }
   ],
   "source": [
    "# train_fold2, test_fold1\n",
    "batch_size = 1\n",
    "rates_fold1 = investigate_model(X.shape[1], 0, 0, batch_size, X, target_fold2, 1, seqs_dict, labels_dict, err_fold1_df, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.6106668896338965 63.26923076923077\n",
      "1 1.750301941467994 70.96153846153847\n",
      "2 1.4002043309624648 76.34615384615384\n",
      "3 1.2817521505981664 79.03846153846153\n",
      "4 1.2339965331524902 79.8076923076923\n",
      "5 1.2152120441692391 79.8076923076923\n",
      "6 1.2090856607559497 79.03846153846153\n",
      "7 1.2063345377845227 79.42307692307692\n",
      "8 1.205349632886509 79.23076923076923\n",
      "9 1.2051810882988563 79.23076923076923\n",
      "10 1.2045974557458707 79.23076923076923\n",
      "11 1.2052642586791322 79.23076923076923\n",
      "12 1.2045526792412777 79.23076923076923\n",
      "13 1.2045702969357737 79.23076923076923\n",
      "14 1.2052218727787816 79.23076923076923\n",
      "15 1.2044591433496663 79.23076923076923\n",
      "16 1.204451192008827 79.23076923076923\n",
      "17 1.2050861298110944 79.23076923076923\n",
      "18 1.2043105521130932 79.23076923076923\n",
      "19 1.20429711878817 79.23076923076923\n",
      "20 1.2042668101050515 79.03846153846153\n"
     ]
    }
   ],
   "source": [
    "# train_fold2, test_fold2 batch 1\n",
    "batch_size = 1\n",
    "rates_fold2_train = investigate_model(X.shape[1], 0, 0, batch_size, X, target_fold2, 2, seqs_dict, labels_dict, err_fold2_df, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4370.904777812034 60.37234042553192\n",
      "1 1239.9685860243028 60.37234042553192\n",
      "2 473.9986102781991 63.962765957446805\n",
      "3 224.0980851924448 64.09574468085107\n",
      "4 120.09973370889766 67.81914893617021\n",
      "5 69.70814097883628 67.81914893617021\n",
      "6 42.74087296585275 67.95212765957447\n",
      "7 27.22994566407082 68.88297872340425\n",
      "8 17.862534931212547 68.48404255319149\n",
      "9 11.99405924945993 69.14893617021276\n",
      "10 8.223657928785524 69.41489361702128\n",
      "11 5.782185261167636 70.07978723404256\n",
      "12 4.204014212908404 70.61170212765957\n",
      "13 3.166317033022191 70.61170212765957\n",
      "14 2.4843900448996217 71.67553191489361\n",
      "15 2.052734138571475 72.34042553191489\n",
      "16 1.781351990122844 73.80319148936171\n",
      "17 1.6125317924815517 74.06914893617021\n",
      "18 1.5051454062292862 75.0\n",
      "19 1.4307364147073605 75.39893617021276\n",
      "20 1.374279949881238 75.39893617021276\n",
      "21 1.34680724921986 76.59574468085107\n",
      "22 1.331515154997073 76.86170212765957\n",
      "23 1.3211521869220886 76.72872340425532\n",
      "24 1.3117199237325396 76.59574468085107\n",
      "25 1.304582190834049 76.72872340425532\n",
      "26 1.2979015631344142 76.99468085106383\n",
      "27 1.2919545225781044 76.99468085106383\n",
      "28 1.2878527750993163 77.26063829787235\n",
      "29 1.2839109048267914 77.3936170212766\n",
      "30 1.280497393456962 78.19148936170212\n",
      "31 1.2771130617212991 78.19148936170212\n",
      "32 1.2740240673432814 80.85106382978724\n",
      "33 1.2684445839529443 80.31914893617021\n",
      "34 1.2652372276710588 80.31914893617021\n",
      "35 1.2609693882730086 80.05319148936171\n",
      "36 1.2560059418989182 78.8563829787234\n",
      "37 1.2509801876133637 78.32446808510639\n",
      "38 1.2447682145693608 78.32446808510639\n",
      "39 1.2426170114533226 78.45744680851064\n",
      "40 1.238968616535833 78.19148936170212\n",
      "41 1.2370212355098644 78.59042553191489\n",
      "42 1.2335818636376785 78.32446808510639\n",
      "43 1.2295625059719966 78.32446808510639\n",
      "44 1.2262294245055103 78.59042553191489\n",
      "45 1.2228483146071472 78.45744680851064\n",
      "46 1.2260747060282455 78.59042553191489\n",
      "47 1.2209324431275312 78.59042553191489\n",
      "48 1.2164073087177498 78.72340425531915\n",
      "49 1.2143316311123566 78.8563829787234\n",
      "50 1.2141617225014016 78.8563829787234\n",
      "51 1.2135891075297902 78.8563829787234\n",
      "52 1.2127947215014532 79.12234042553192\n",
      "53 1.2106074771607638 78.98936170212765\n",
      "54 1.211405820106004 78.98936170212765\n",
      "55 1.2106437175389322 79.12234042553192\n",
      "56 1.2092201021634106 79.12234042553192\n",
      "57 1.208217923569825 78.98936170212765\n",
      "58 1.2077865360771256 78.72340425531915\n",
      "59 1.2058989476132327 78.8563829787234\n",
      "60 1.2053305469540674 78.8563829787234\n",
      "61 1.2050184797319774 79.25531914893617\n",
      "62 1.2047157747704897 79.12234042553192\n",
      "63 1.2046099701063053 79.38829787234043\n",
      "64 1.2044310465366121 79.38829787234043\n",
      "65 1.204137806969621 79.52127659574468\n",
      "66 1.2042492995937601 79.65425531914893\n",
      "67 1.2043971391872457 79.7872340425532\n",
      "68 1.203873692424878 79.7872340425532\n",
      "69 1.2039688081718543 79.7872340425532\n",
      "70 1.203881743551435 79.7872340425532\n",
      "71 1.2046001804878845 79.7872340425532\n",
      "72 1.204581989699502 79.7872340425532\n",
      "73 1.2045966849293601 79.7872340425532\n",
      "74 1.203799968260289 79.7872340425532\n",
      "75 1.2045577815186093 79.7872340425532\n",
      "76 1.2037954489190201 79.7872340425532\n",
      "77 1.2045247445093328 79.7872340425532\n",
      "78 1.204527617161725 79.7872340425532\n",
      "79 1.2037384380940699 79.7872340425532\n",
      "80 1.2045005201967411 79.7872340425532\n",
      "81 1.20372618079882 79.7872340425532\n",
      "82 1.2044614758990269 79.7872340425532\n",
      "83 1.204438840342367 79.7872340425532\n",
      "84 1.2044978207076777 79.7872340425532\n",
      "85 1.202113159359394 79.7872340425532\n",
      "86 1.2045259022791237 79.7872340425532\n",
      "87 1.2045018597990773 79.7872340425532\n",
      "88 1.2044214933035422 79.7872340425532\n",
      "89 1.2044111501532937 79.7872340425532\n",
      "90 1.2044770381589698 79.7872340425532\n",
      "91 1.2048217320422838 79.7872340425532\n",
      "92 1.2044371454314644 79.7872340425532\n",
      "93 1.2044716359828962 79.7872340425532\n",
      "94 1.2041971314387168 79.7872340425532\n",
      "95 1.2044144305673476 79.7872340425532\n",
      "96 1.202093581983003 79.7872340425532\n",
      "97 1.2045337403255705 79.7872340425532\n",
      "98 1.2044345390530078 79.7872340425532\n",
      "99 1.2045120085820284 79.7872340425532\n",
      "100 1.2042052482953582 79.7872340425532\n"
     ]
    }
   ],
   "source": [
    "# train_fold2, test_fold1\n",
    "batch_size = 1\n",
    "rates_fold1 = investigate_model(X.shape[1], 2, 16, batch_size, X, target_fold2, 1, seqs_dict, labels_dict, err_fold1_df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4370.904777812034 27.884615384615383\n",
      "1 1239.9685860243028 27.884615384615383\n",
      "2 473.9986102781991 39.03846153846154\n",
      "3 224.0980851924448 40.19230769230769\n",
      "4 120.09973370889766 56.34615384615385\n",
      "5 69.70814097883628 57.30769230769231\n",
      "6 42.74087296585275 58.26923076923077\n",
      "7 27.22994566407082 61.53846153846154\n",
      "8 17.862534931212547 62.5\n",
      "9 11.99405924945993 63.26923076923077\n",
      "10 8.223657928785524 64.42307692307692\n",
      "11 5.782185261167636 66.15384615384616\n",
      "12 4.204014212908404 66.92307692307692\n",
      "13 3.166317033022191 68.26923076923077\n",
      "14 2.4843900448996217 69.8076923076923\n",
      "15 2.052734138571475 70.96153846153847\n",
      "16 1.781351990122844 72.5\n",
      "17 1.6125317924815517 74.03846153846153\n",
      "18 1.5051454062292862 74.42307692307692\n",
      "19 1.4307364147073605 75.76923076923077\n",
      "20 1.374279949881238 76.34615384615384\n",
      "21 1.34680724921986 75.96153846153847\n",
      "22 1.331515154997073 76.15384615384616\n",
      "23 1.3211521869220886 76.15384615384616\n",
      "24 1.3117199237325396 75.96153846153847\n",
      "25 1.304582190834049 76.34615384615384\n",
      "26 1.2979015631344142 76.73076923076923\n",
      "27 1.2919545225781044 76.92307692307692\n",
      "28 1.2878527750993163 76.92307692307692\n",
      "29 1.2839109048267914 77.11538461538461\n",
      "30 1.280497393456962 76.92307692307692\n",
      "31 1.2771130617212991 77.11538461538461\n",
      "32 1.2740240673432814 75.76923076923077\n",
      "33 1.2684445839529443 76.53846153846153\n",
      "34 1.2652372276710588 76.53846153846153\n",
      "35 1.2609693882730086 76.15384615384616\n",
      "36 1.2560059418989182 78.46153846153847\n",
      "37 1.2509801876133637 78.07692307692308\n",
      "38 1.2447682145693608 77.88461538461539\n",
      "39 1.2426170114533226 76.92307692307692\n",
      "40 1.238968616535833 76.73076923076923\n",
      "41 1.2370212355098644 77.5\n",
      "42 1.2335818636376785 76.92307692307692\n",
      "43 1.2295625059719966 77.11538461538461\n",
      "44 1.2262294245055103 77.3076923076923\n",
      "45 1.2228483146071472 77.11538461538461\n",
      "46 1.2260747060282455 77.88461538461539\n",
      "47 1.2209324431275312 77.88461538461539\n",
      "48 1.2164073087177498 77.6923076923077\n",
      "49 1.2143316311123566 77.6923076923077\n",
      "50 1.2141617225014016 77.6923076923077\n"
     ]
    }
   ],
   "source": [
    "# train_fold2, test_fold2 batch 1\n",
    "batch_size = 1\n",
    "rates_fold2_train = investigate_model(X.shape[1], 2, 16, batch_size, X, target_fold2, 2, seqs_dict, labels_dict, err_fold2_df, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 23.442506821442183 64.36170212765957\n",
      "1 8.81400370057537 69.41489361702128\n",
      "2 3.8433412979895505 71.27659574468085\n",
      "3 2.1479744518407706 75.26595744680851\n",
      "4 1.5500484322795034 77.52659574468085\n",
      "5 1.3512789674070174 79.7872340425532\n",
      "6 1.293065024993087 80.71808510638297\n",
      "7 1.272657644738519 80.05319148936171\n",
      "8 1.2648096946772802 79.65425531914893\n",
      "9 1.2614717148514982 79.92021276595744\n",
      "10 1.2578839894889269 79.52127659574468\n",
      "11 1.2535935243305278 79.65425531914893\n",
      "12 1.2491545511892215 79.92021276595744\n",
      "13 1.2450693000788244 80.31914893617021\n",
      "14 1.2407217425785815 80.31914893617021\n",
      "15 1.2374924437831254 79.7872340425532\n",
      "16 1.2338458440890132 79.65425531914893\n",
      "17 1.2309909267133985 79.12234042553192\n",
      "18 1.2283610465822898 78.45744680851064\n",
      "19 1.2231128996743534 78.72340425531915\n",
      "20 1.216316051281409 78.05851063829788\n",
      "21 1.2152587148475351 78.32446808510639\n",
      "22 1.2138637805396475 78.45744680851064\n",
      "23 1.2131646958153839 78.45744680851064\n",
      "24 1.2111834903506207 78.59042553191489\n",
      "25 1.2106976204250552 78.59042553191489\n",
      "26 1.2110383424776991 78.59042553191489\n",
      "27 1.2101101755934371 78.72340425531915\n",
      "28 1.2074029129816677 78.98936170212765\n",
      "29 1.206894414301524 78.98936170212765\n",
      "30 1.2075038048208715 78.59042553191489\n",
      "31 1.2070870346100222 78.59042553191489\n",
      "32 1.2060178337754293 78.59042553191489\n",
      "33 1.2048091398127554 78.8563829787234\n",
      "34 1.2047232427750547 78.72340425531915\n",
      "35 1.2042966737078262 78.32446808510639\n",
      "36 1.2034542074858465 78.32446808510639\n",
      "37 1.2018680068840233 78.72340425531915\n",
      "38 1.2026835225744432 78.45744680851064\n",
      "39 1.2027952487643863 78.45744680851064\n",
      "40 1.2026257929420412 78.59042553191489\n",
      "41 1.202758113595447 78.59042553191489\n",
      "42 1.2028619570181742 78.45744680851064\n",
      "43 1.2028629801789583 78.45744680851064\n",
      "44 1.2022542769135827 78.59042553191489\n",
      "45 1.2026158148538653 78.72340425531915\n",
      "46 1.2021811748112603 78.72340425531915\n",
      "47 1.2017162359520444 78.72340425531915\n",
      "48 1.2018915040452478 78.98936170212765\n",
      "49 1.2016566458006614 78.98936170212765\n",
      "50 1.2015915467381968 78.98936170212765\n",
      "51 1.2015314093321514 78.98936170212765\n",
      "52 1.2015611236055352 78.98936170212765\n",
      "53 1.2015283811783548 78.98936170212765\n",
      "54 1.201970157222054 78.98936170212765\n",
      "55 1.2018777923592625 78.98936170212765\n",
      "56 1.2018611021219705 78.98936170212765\n",
      "57 1.2012896473256507 78.98936170212765\n",
      "58 1.2019357424267798 78.98936170212765\n",
      "59 1.201841502711254 78.98936170212765\n",
      "60 1.2016150347122307 78.98936170212765\n",
      "61 1.2017776800178122 78.98936170212765\n",
      "62 1.2017432888823119 78.98936170212765\n",
      "63 1.2012565799847132 78.98936170212765\n",
      "64 1.2019371322388874 78.98936170212765\n",
      "65 1.2016132357652745 78.98936170212765\n",
      "66 1.2016642016735348 78.98936170212765\n",
      "67 1.2015456065786139 78.98936170212765\n",
      "68 1.2017869917734307 78.98936170212765\n",
      "69 1.2020483627690504 78.98936170212765\n",
      "70 1.201632214253831 78.98936170212765\n",
      "71 1.2016191835082282 78.98936170212765\n",
      "72 1.201610289859392 78.98936170212765\n",
      "73 1.2015989255626511 78.98936170212765\n",
      "74 1.2013813150251538 78.98936170212765\n",
      "75 1.2019268915810086 78.98936170212765\n",
      "76 1.2015587024361731 78.98936170212765\n",
      "77 1.2013427252986764 78.98936170212765\n",
      "78 1.2018619429916189 78.98936170212765\n",
      "79 1.2016635998060674 78.98936170212765\n",
      "80 1.2014743124430027 78.98936170212765\n",
      "81 1.201348804973211 78.98936170212765\n",
      "82 1.2017707108419708 78.98936170212765\n",
      "83 1.2014728755204922 78.98936170212765\n",
      "84 1.2014704386007562 78.98936170212765\n",
      "85 1.2012523385078466 78.98936170212765\n",
      "86 1.2017702687776333 78.98936170212765\n",
      "87 1.2014595722619403 78.98936170212765\n",
      "88 1.2014253186417139 78.98936170212765\n",
      "89 1.2011890629514952 78.98936170212765\n",
      "90 1.201820577534147 78.98936170212765\n",
      "91 1.2011713408381965 78.98936170212765\n",
      "92 1.2016843984750005 78.98936170212765\n",
      "93 1.2011698489025033 78.98936170212765\n",
      "94 1.2016784475530125 78.98936170212765\n",
      "95 1.2013930575651657 78.98936170212765\n",
      "96 1.2010819633776861 78.98936170212765\n",
      "97 1.2016512139427655 78.98936170212765\n",
      "98 1.2012282058974322 78.98936170212765\n",
      "99 1.2016686002902803 78.98936170212765\n",
      "100 1.2010388189167824 78.98936170212765\n"
     ]
    }
   ],
   "source": [
    "# train_fold2, test_fold1\n",
    "batch_size = 1\n",
    "rates_fold1 = investigate_model(X.shape[1], 1, 8, batch_size, X, target_fold2, 1, seqs_dict, labels_dict, err_fold1_df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 23.442506821442183 42.11538461538461\n",
      "1 8.81400370057537 59.23076923076923\n",
      "2 3.8433412979895505 63.84615384615385\n",
      "3 2.1479744518407706 68.84615384615384\n",
      "4 1.5500484322795034 72.11538461538461\n",
      "5 1.3512789674070174 75.38461538461539\n",
      "6 1.293065024993087 75.96153846153847\n",
      "7 1.272657644738519 78.07692307692308\n",
      "8 1.2648096946772802 78.26923076923077\n",
      "9 1.2614717148514982 78.65384615384616\n",
      "10 1.2578839894889269 78.46153846153847\n",
      "11 1.2535935243305278 78.46153846153847\n",
      "12 1.2491545511892215 78.84615384615384\n",
      "13 1.2450693000788244 79.03846153846153\n",
      "14 1.2407217425785815 79.03846153846153\n",
      "15 1.2374924437831254 78.84615384615384\n",
      "16 1.2338458440890132 78.65384615384616\n",
      "17 1.2309909267133985 78.65384615384616\n",
      "18 1.2283610465822898 79.03846153846153\n",
      "19 1.2231128996743534 77.88461538461539\n",
      "20 1.216316051281409 77.6923076923077\n",
      "21 1.2152587148475351 77.88461538461539\n",
      "22 1.2138637805396475 77.6923076923077\n",
      "23 1.2131646958153839 78.07692307692308\n",
      "24 1.2111834903506207 77.6923076923077\n",
      "25 1.2106976204250552 77.6923076923077\n",
      "26 1.2110383424776991 77.6923076923077\n",
      "27 1.2101101755934371 77.6923076923077\n",
      "28 1.2074029129816677 77.6923076923077\n",
      "29 1.206894414301524 77.5\n",
      "30 1.2075038048208715 77.6923076923077\n",
      "31 1.2070870346100222 77.88461538461539\n",
      "32 1.2060178337754293 78.07692307692308\n",
      "33 1.2048091398127554 77.6923076923077\n",
      "34 1.2047232427750547 77.88461538461539\n",
      "35 1.2042966737078262 77.88461538461539\n",
      "36 1.2034542074858465 77.88461538461539\n",
      "37 1.2018680068840233 77.3076923076923\n",
      "38 1.2026835225744432 77.3076923076923\n",
      "39 1.2027952487643863 77.3076923076923\n",
      "40 1.2026257929420412 77.3076923076923\n",
      "41 1.202758113595447 77.3076923076923\n",
      "42 1.2028619570181742 77.3076923076923\n",
      "43 1.2028629801789583 77.3076923076923\n",
      "44 1.2022542769135827 77.3076923076923\n",
      "45 1.2026158148538653 77.3076923076923\n",
      "46 1.2021811748112603 77.6923076923077\n",
      "47 1.2017162359520444 77.3076923076923\n",
      "48 1.2018915040452478 77.88461538461539\n",
      "49 1.2016566458006614 77.88461538461539\n",
      "50 1.2015915467381968 77.88461538461539\n"
     ]
    }
   ],
   "source": [
    "# train_fold2, test_fold2 batch 1\n",
    "batch_size = 1\n",
    "rates_fold2_train = investigate_model(X.shape[1], 1, 8, batch_size, X, target_fold2, 2, seqs_dict, labels_dict, err_fold2_df, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
