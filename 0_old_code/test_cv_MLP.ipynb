{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from opart_functions import SquaredHingeLoss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from opart_functions import get_acc_rate, get_err_df, gen_data_dict, tune_lldas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "features_fold1_path = 'training_data/genome/seq_features.csv'\n",
    "features_fold2_path = 'training_data/genome/seq_features.csv'  \n",
    "target_fold1_path = 'training_data/genome/target_fold1.csv'\n",
    "target_fold2_path = 'training_data/genome/target_fold2.csv'\n",
    "\n",
    "# sequences and labels\n",
    "seqs_path   = 'raw_data/genome/signals.csv'\n",
    "labels_path = 'raw_data/genome/labels.csv'\n",
    "\n",
    "# err for each log_lambda\n",
    "err_fold1_path = 'training_data/genome/errors_fold1.csv'\n",
    "err_fold2_path = 'training_data/genome/errors_fold2.csv'\n",
    "\n",
    "# writing accuracy rate path\n",
    "acc_rate_path = 'acc_rate/genome.csv'\n",
    "\n",
    "# path to write df to csv\n",
    "output_df_path = 'record_dataframe/genome/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sequence and label dictionary\n",
    "seqs_dict   = gen_data_dict(seqs_path)\n",
    "labels_dict = gen_data_dict(labels_path)\n",
    "\n",
    "# getting dataframe of error count for each log_lambda\n",
    "err_fold1_df = pd.read_csv(err_fold1_path)\n",
    "err_fold2_df = pd.read_csv(err_fold2_path)\n",
    "\n",
    "# features_df\n",
    "features_df_fold1 = pd.read_csv(features_fold1_path)\n",
    "features_df_fold2 = pd.read_csv(features_fold2_path)\n",
    "\n",
    "# targets_df\n",
    "target_df_fold1 = pd.read_csv(target_fold1_path)\n",
    "target_df_fold2 = pd.read_csv(target_fold2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature\n",
    "chosen_feature = ['std_deviation', 'length', 'sum_diff', 'range_value', 'abs_skewness']\n",
    "X = features_df_fold1.iloc[:, 1:][chosen_feature].to_numpy()\n",
    "X0 = X[:, 0]\n",
    "X0 = np.log(X0).reshape(-1, 1)\n",
    "X1 = X[:, 1]\n",
    "X1 = np.log(np.log(X1)).reshape(-1, 1)\n",
    "X2 = X[:, 2]\n",
    "X2 = np.log(np.log(X2)).reshape(-1, 1)\n",
    "X3 = X[:, 3]\n",
    "X3 = np.log(X3).reshape(-1, 1)\n",
    "X4 = X[:, 4]\n",
    "X4 = np.log(X4).reshape(-1, 1)\n",
    "\n",
    "X = np.concatenate([X0, X1, X2, X3, X4], axis=1)\n",
    "mean = np.mean(X, axis=0)\n",
    "std_dev = np.std(X, axis=0)\n",
    "X = (X-mean)/std_dev\n",
    "X = torch.Tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_low_1  = torch.Tensor(target_df_fold1.iloc[:, 1:2].to_numpy())\n",
    "targets_high_1 = torch.Tensor(target_df_fold1.iloc[:, 2:3].to_numpy())\n",
    "targets_low_2  = torch.Tensor(target_df_fold2.iloc[:, 1:2].to_numpy())\n",
    "targets_high_2 = torch.Tensor(target_df_fold2.iloc[:, 2:3].to_numpy())\n",
    "\n",
    "target_fold1 = torch.cat((targets_low_1, targets_high_1), dim=1)\n",
    "target_fold2 = torch.cat((targets_low_2, targets_high_2), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    torch.manual_seed(123)\n",
    "    def __init__(self, input_size, hidden_layers, hidden_size):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.input_size    = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_size   = hidden_size\n",
    "\n",
    "        if(self.hidden_layers == 0):\n",
    "            self.linear_model = nn.Linear(input_size, 1)                                                        # Define linear model\n",
    "        else:\n",
    "            self.input_layer = nn.Linear(input_size, hidden_size)                                               # Define input layer\n",
    "            self.hidden = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(hidden_layers-1)])  # Define hidden layers\n",
    "            self.output_layer = nn.Linear(hidden_size, 1)                                                       # Define output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if(self.hidden_layers == 0):\n",
    "            return self.linear_model(x)\n",
    "        else:\n",
    "            x = torch.sigmoid(self.input_layer(x))\n",
    "            for layer in self.hidden:\n",
    "                x = torch.sigmoid(layer(x))\n",
    "            x = self.output_layer(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot train loss and val loss\n",
    "def plot_loss(train_loss, val_loss, best_ite, train_set_name, val_set_name):\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss,   'r', label='Validation loss')\n",
    "    \n",
    "    # Mark the minimum validation loss point\n",
    "    if(best_ite != None):\n",
    "        plt.plot(best_ite, val_loss[best_ite], 'g*', markersize=10, label=f'Min Val epoch: {best_ite: 3d}')\n",
    "\n",
    "    plt.title('Train ' + train_set_name + \" Validate \" + val_set_name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_learn(n_splits, X, y, n_hiddens, layer_size, batch_size, n_ite, show_plot):\n",
    "    torch.manual_seed(123)\n",
    "    \n",
    "    # Define the number of folds for cross-validation\n",
    "    kf = KFold(n_splits, shuffle=True, random_state=123)\n",
    "\n",
    "    # loss function\n",
    "    loss_func = SquaredHingeLoss(margin=1)\n",
    "\n",
    "    # learn best ite\n",
    "    total_train_losses = np.zeros(n_ite)\n",
    "    total_val_losses   = np.zeros(n_ite)\n",
    "    for train_index, val_index in kf.split(X):\n",
    "\n",
    "        # Split the data into training and validation sets\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset    = TensorDataset(X_train, y_train)\n",
    "        dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "        # Define your model\n",
    "        model = MLPModel(X.shape[1], n_hiddens, layer_size)\n",
    "\n",
    "        # define optimizer\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "        # Training loop for the specified number of iterations\n",
    "        train_losses = []\n",
    "        val_losses   = []\n",
    "        for i in range(n_ite):\n",
    "            # training\n",
    "            train_loss = 0\n",
    "            for inputs, labels in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_func(model(inputs), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # validating\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = loss_func(model(X_val), y_val)\n",
    "\n",
    "            # add train_loss and val_loss into arrays\n",
    "            train_losses.append(train_loss/len(dataloader))\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        total_train_losses += train_losses\n",
    "        total_val_losses += val_losses\n",
    "\n",
    "    best_no_ite = np.argmin(total_val_losses)\n",
    "    if(show_plot == True):\n",
    "        plot_loss(total_train_losses/n_splits, total_val_losses/n_splits, best_no_ite, 'subtrain', 'val')\n",
    "    return best_no_ite + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn lldas\n",
    "def mlp(features, targets, hidden_layers, hidden_size, batch_size, n_ites, test_fold, err_df):\n",
    "    torch.manual_seed(123)\n",
    "    # prepare training dataset\n",
    "    dataset    = TensorDataset(features, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "    # Instantiate model, loss function and opimizer\n",
    "    model = MLPModel(features.shape[1], hidden_layers, hidden_size)\n",
    "    criterion = SquaredHingeLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(n_ites):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            lldas = model(features).numpy().reshape(-1)\n",
    "\n",
    "            lldas = tune_lldas(lldas)\n",
    "            df = get_err_df(lldas, test_fold, seqs_dict, labels_dict, err_df)\n",
    "            rate = get_acc_rate(df)\n",
    "\n",
    "        print(i, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn n_o_ite for training fold 2\n",
    "cv_learn(2, X, target_fold1, 0, 0, 1, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn n_o_ite for training fold 2\n",
    "cv_learn(2, X, target_fold2, 1, 8, 1, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn n_o_ite for training fold 2\n",
    "cv_learn(2, X, target_fold2, 0, 0, 1, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn n_o_ite for training fold 1\n",
    "cv_learn(2, X, target_fold2, 2, 16, 1, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train fold2, test fold1 batch 1\n",
    "mlp(X, target_fold2, 2, 16, 1, 30, 1, err_fold1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train fold2, test fold2 batch 1\n",
    "mlp(X, target_fold2, 2, 16, 1, 10, 2, err_fold2_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
